{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sorted-programming",
   "metadata": {},
   "source": [
    "# sorafune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "separate-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import typing as tp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from matplotlib_venn import venn2\n",
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import gensim\n",
    "\n",
    "from collections import Counter\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "\n",
    "from catboost import CatBoost\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool\n",
    "from catboost import cv\n",
    "import optuna\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from xfeat import (SelectCategorical, LabelEncoder, Pipeline, ConcatCombination, SelectNumerical, \n",
    "                   ArithmeticCombinations, TargetEncoder, aggregation, GBDTFeatureSelector, GBDTFeatureExplorer)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "thermal-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../input/\"\n",
    "output_dir = \"../output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "following-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from time import time\n",
    "\n",
    "@contextmanager\n",
    "def timer(logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None):\n",
    "    if prefix: format_str = str(prefix) + format_str\n",
    "    if suffix: format_str = format_str + str(suffix)\n",
    "    start = time()\n",
    "    yield\n",
    "    d = time() - start\n",
    "    out_str = format_str.format(d)\n",
    "    if logger:\n",
    "        logger.info(out_str)\n",
    "    else:\n",
    "        print(out_str)\n",
    "        \n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "answering-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(input_dir+\"TrainDataSet.csv\")\n",
    "test_df = pd.read_csv(input_dir+\"EvaluationData.csv\")\n",
    "submission = pd.read_csv(input_dir+\"UploadFileTemplate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "divine-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregationのagg_methodsで使用\n",
    "def max_min(x):\n",
    "    return x.max()-x.min()\n",
    "\n",
    "def q75_q25(x):\n",
    "    return x.quantile(0.75) - x.quantile(0.25)\n",
    "\n",
    "def q25(x):\n",
    "    return x.quantile(0.25)\n",
    "\n",
    "def q50(x):\n",
    "    return x.quantile(0.5)\n",
    "def q75(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "def aggregation_cumfeat(input_df, group_key, group_values):\n",
    "    output_df = pd.DataFrame()\n",
    "    for col in group_values:\n",
    "        if input_df.AverageLandPrice.min() ==1:\n",
    "            new_col = f\"cum_feat_{col}_grpby_{group_key}\"\n",
    "        else:\n",
    "            new_col = f\"all_cum_feat_{col}_grpby_{group_key}\"\n",
    "        input_df[\"lag\"] = input_df.groupby(group_key)[[col]].shift(1)\n",
    "        cum = input_df[[group_key]+[\"lag\"]].groupby(group_key).lag.agg([\"cumsum\", \"cumcount\"])\n",
    "        new_df = pd.DataFrame(cum[\"cumsum\"]/cum[\"cumcount\"])\n",
    "        new_df.columns = [new_col]\n",
    "        output_df = pd.concat([output_df, new_df],axis=1)\n",
    "            \n",
    "    return output_df\n",
    "\n",
    "def get_agg_cumfeat_features(input_df):\n",
    "    _input_df =  pd.concat([input_df, get_area_feature(input_df)], axis=1)\n",
    "    \n",
    "    group_key = \"PlaceID\"\n",
    "    group_values = [\"MeanLight\",\"SumLight\"]    \n",
    "    output_df = aggregation_cumfeat(input_df,\n",
    "                                    group_key=group_key,\n",
    "                                    group_values=group_values)\n",
    "    return output_df    \n",
    "\n",
    "# group 内で diffをとる関数\n",
    "def diff_aggregation(input_df, group_key, group_values, num_diffs):\n",
    "    dfs = []\n",
    "    for nd in num_diffs:\n",
    "        _df = input_df.groupby(group_key)[group_values].diff(nd)\n",
    "        _df.columns = [f'diff={nd}_{col}_grpby_{group_key}' for col in group_values]\n",
    "        dfs.append(_df)\n",
    "    output_df = pd.concat(dfs, axis=1)\n",
    "    return output_df\n",
    "\n",
    "# group 内で shiftをとる関数\n",
    "def shift_aggregation(input_df, group_key, group_values, num_shifts):\n",
    "    dfs = []\n",
    "    for ns in num_shifts:\n",
    "        _df = input_df.groupby(group_key)[group_values].shift(ns)\n",
    "        _df.columns = [f'shift={ns}_{col}_grpby_{group_key}' for col in group_values]\n",
    "        dfs.append(_df)\n",
    "    output_df = pd.concat(dfs, axis=1)\n",
    "    return output_df\n",
    "\n",
    "# そのままの値の特徴量\n",
    "def get_raw_features(input_df):\n",
    "    cols = [\n",
    "        \"MeanLight\",\n",
    "        \"SumLight\",\n",
    "        \"Year\"\n",
    "    ]\n",
    "    return input_df[cols].copy()\n",
    "\n",
    "# 面積\n",
    "def get_area_feature(input_df):\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[\"Area\"] = input_df[\"SumLight\"] / (input_df[\"MeanLight\"]+1e-3)\n",
    "    return output_df\n",
    "\n",
    "# aggration PlaceID\n",
    "def get_agg_place_id_features(input_df):\n",
    "    _input_df = pd.concat([input_df, get_area_feature(input_df)], axis=1)\n",
    "    \n",
    "    cols = 'PlaceID'\n",
    "\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df, agg_cols = aggregation(_input_df,\n",
    "                                      group_key=cols,\n",
    "                                      group_values=[\"MeanLight\", \"SumLight\", \"Area\"],\n",
    "                                      agg_methods=[\"min\", \"max\", \"median\", \"mean\", \"std\",\"var\", max_min, q75_q25,q25,q50,q75],\n",
    "                                               )\n",
    "    \n",
    "    return output_df[agg_cols]\n",
    "\n",
    "# aggration Year\n",
    "def get_agg_year_features(input_df):\n",
    "    _input_df = pd.concat([input_df, get_area_feature(input_df)], axis=1)\n",
    "    \n",
    "    cols = 'Year'\n",
    "\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df, agg_cols = aggregation(_input_df,\n",
    "                                      group_key=cols,\n",
    "                                      group_values=[\"MeanLight\", \"SumLight\", \"Area\"],\n",
    "                                      agg_methods=[\"min\", \"max\", \"median\", \"mean\", \"std\", \"var\",max_min, q75_q25,q25,q50,q75],\n",
    "                                               )\n",
    "    \n",
    "    return output_df[agg_cols]\n",
    "\n",
    "# PlaceID をキーにしたグループ内差分\n",
    "def get_diff_agg_place_id_features(input_df):\n",
    "    group_key = \"PlaceID\"\n",
    "    group_values = [\"MeanLight\", \"SumLight\"]\n",
    "    num_diffs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
    "                 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "    output_df = diff_aggregation(input_df, \n",
    "                                 group_key=group_key, \n",
    "                                 group_values=group_values, \n",
    "                                 num_diffs=num_diffs)\n",
    "    return output_df\n",
    "\n",
    "# PlaceID をキーにしたグループ内シフト\n",
    "def get_shift_agg_place_id_features(input_df):\n",
    "    group_key = \"PlaceID\"\n",
    "    group_values = [\"MeanLight\", \"SumLight\"]\n",
    "    num_shifts = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
    "                 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "    output_df = shift_aggregation(input_df, \n",
    "                                  group_key=group_key, \n",
    "                                  group_values=group_values, \n",
    "                                  num_shifts=num_shifts)\n",
    "    return output_df\n",
    "\n",
    "# pivot tabel を用いた特徴量\n",
    "def get_place_id_vecs_features(input_df):\n",
    "    _input_df = pd.concat([input_df, get_area_feature(input_df)], axis=1)\n",
    "    # pivot table\n",
    "    area_df = pd.pivot_table(_input_df, index=\"PlaceID\", columns=\"Year\", values=\"Area\").add_prefix(\"Area=\")\n",
    "    mean_light_df = pd.pivot_table(_input_df, index=\"PlaceID\", columns=\"Year\", values=\"MeanLight\").add_prefix(\"MeanLight=\")\n",
    "    sum_light_df = pd.pivot_table(_input_df, index=\"PlaceID\", columns=\"Year\", values=\"SumLight\").add_prefix(\"SumLight=\")\n",
    "    all_df = pd.concat([area_df, mean_light_df, sum_light_df], axis=1)\n",
    "    \n",
    "    # PCA all \n",
    "    sc_all_df = StandardScaler().fit_transform(all_df.fillna(0))\n",
    "    pca = PCA(n_components=64, random_state=2021)\n",
    "    pca_all_df = pd.DataFrame(pca.fit_transform(sc_all_df), index=all_df.index).rename(columns=lambda x: f\"PlaceID_all_PCA={x:03}\")\n",
    "    # PCA Area\n",
    "    sc_area_df = StandardScaler().fit_transform(area_df.fillna(0))\n",
    "    pca = PCA(n_components=16, random_state=2021)\n",
    "    pca_area_df = pd.DataFrame(pca.fit_transform(sc_area_df), index=all_df.index).rename(columns=lambda x: f\"PlaceID_Area_PCA={x:03}\")\n",
    "    # PCA MeanLight\n",
    "    sc_mean_light_df = StandardScaler().fit_transform(mean_light_df.fillna(0))\n",
    "    pca = PCA(n_components=16, random_state=2021)\n",
    "    pca_mean_light_df = pd.DataFrame(pca.fit_transform(sc_mean_light_df), index=all_df.index).rename(columns=lambda x: f\"PlaceID_MeanLight_PCA={x:03}\")\n",
    "    # PCA SumLight\n",
    "    sc_sum_light_df = StandardScaler().fit_transform(sum_light_df.fillna(0))\n",
    "    pca = PCA(n_components=16, random_state=2021)\n",
    "    pca_sum_light_df = pd.DataFrame(pca.fit_transform(sc_sum_light_df), index=all_df.index).rename(columns=lambda x: f\"PlaceID_SumLight_PCA={x:03}\")\n",
    "    \n",
    "    df = pd.concat([all_df, pca_all_df, pca_area_df, pca_mean_light_df, pca_sum_light_df], axis=1)\n",
    "    output_df = pd.merge(_input_df[[\"PlaceID\"]], df, left_on=\"PlaceID\", right_index=True, how=\"left\")\n",
    "    return output_df.drop(\"PlaceID\", axis=1)\n",
    "\n",
    "# PlaceIDをキーにしたグループ内相関係数\n",
    "def get_corr_features(input_df):\n",
    "    _input_df = pd.concat([input_df, get_area_feature(input_df)], axis=1)\n",
    "    group_key = \"PlaceID\"\n",
    "    group_vlaues = [\n",
    "        [\"Year\", \"MeanLight\"],\n",
    "        [\"Year\", \"SumLight\"],\n",
    "        [\"Year\", \"Area\"],\n",
    "    ]\n",
    "    dfs = []\n",
    "    for gv in group_vlaues:\n",
    "        _df = _input_df.groupby(group_key)[gv].corr().unstack().iloc[:, 1].rename(f\"Corr={gv[0]}-{gv[1]}\")\n",
    "        dfs.append(pd.DataFrame(_df))\n",
    "    dfs = pd.concat(dfs, axis=1)\n",
    "    output_df = pd.merge(_input_df[[group_key]], dfs, left_on=group_key, right_index=True, how=\"left\").drop(group_key, axis=1)\n",
    "    return output_df\n",
    "    \n",
    "# count 63\n",
    "def get_count63_feature(input_df):\n",
    "    # 各地域でMeanLightが63をとった回数を特徴量にする\n",
    "    _mapping = input_df[input_df['MeanLight']==63].groupby('PlaceID').size()\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    output_df['count63'] = input_df['PlaceID'].map(_mapping).fillna(0)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "spatial-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理関数を順々に処理していく関数\n",
    "def to_features(train, test):\n",
    "    input_df = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "    processes = [\n",
    "        get_raw_features,\n",
    "        get_area_feature,\n",
    "        get_agg_place_id_features,\n",
    "        get_agg_year_features,\n",
    "        get_diff_agg_place_id_features,\n",
    "        get_shift_agg_place_id_features,\n",
    "        get_place_id_vecs_features,\n",
    "        get_corr_features,\n",
    "        get_count63_feature,\n",
    "        get_agg_cumfeat_features\n",
    "    ]\n",
    "\n",
    "    output_df = pd.DataFrame()\n",
    "    for func in tqdm(processes):\n",
    "        _df = func(input_df)\n",
    "        assert len(_df) == len(input_df), func.__name__\n",
    "        output_df = pd.concat([output_df, _df], axis=1)\n",
    "\n",
    "    train_x = output_df.iloc[:len(train)] \n",
    "    test_x = output_df.iloc[len(train):].reset_index(drop=True)\n",
    "    return train_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "instructional-civilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:19<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "target_data = \"AverageLandPrice\" \n",
    "\n",
    "train_x, test_x = to_features(train_df, test_df)\n",
    "train_ys = train_df[target_data]\n",
    "train_ys = np.log1p(train_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "stone-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lgbm(X, y, cv, params: dict=None, verbose: int=50):\n",
    "    metric_func = mean_squared_error\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    models = []\n",
    "    oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "\n",
    "    for i, (idx_train, idx_valid) in enumerate(cv): \n",
    "        x_train, y_train = X[idx_train], y[idx_train]\n",
    "        x_valid, y_valid = X[idx_valid], y[idx_valid]\n",
    "\n",
    "        clf = lgb.LGBMRegressor(**params)\n",
    "\n",
    "        with timer(prefix='fit fold={} '.format(i + 1)):\n",
    "            clf.fit(x_train, y_train, \n",
    "                    eval_set=[(x_valid, y_valid)],  \n",
    "                    early_stopping_rounds=verbose,\n",
    "                    verbose=verbose)\n",
    "\n",
    "        pred_i = clf.predict(x_valid)\n",
    "\n",
    "        oof_pred[idx_valid] = pred_i\n",
    "        models.append(clf)\n",
    "\n",
    "        print(f'Fold {i} RMSLE: {metric_func(y_valid, pred_i)**.5 :.4f}')\n",
    "\n",
    "    score = metric_func(y, oof_pred)**.5 \n",
    "    print('FINISHED | Whole RMSLE: {:.4f}'.format(score))\n",
    "    return oof_pred, models\n",
    "\n",
    "# XGB\n",
    "def fit_xgb(X, y, cv, params: dict=None, verbose: int=50):\n",
    "    metric_func = mean_squared_error\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    models = []\n",
    "    oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "\n",
    "    for i, (idx_train, idx_valid) in enumerate(cv): \n",
    "        x_train, y_train = X[idx_train], y[idx_train]\n",
    "        x_valid, y_valid = X[idx_valid], y[idx_valid]\n",
    "        \n",
    "        model_xgb = xgb.XGBRegressor(**params)\n",
    "\n",
    "        with timer(prefix='fit fold={} '.format(i + 1)):\n",
    "            model_xgb.fit(x_train, y_train, eval_set=[(x_valid, y_valid)],verbose=-1)\n",
    "            \n",
    "        #print(model_xgb.best_score())\n",
    "        \n",
    "        pred_i = model_xgb.predict(x_valid)\n",
    "\n",
    "        oof_pred[idx_valid] = pred_i\n",
    "        models.append(model_xgb)\n",
    "\n",
    "        print(f'Fold {i} RMSLE: {metric_func(y_valid, pred_i)**.5 :.4f}')\n",
    "\n",
    "    score = metric_func(y, oof_pred)**.5 \n",
    "    print('FINISHED | Whole RMSLE: {:.4f}'.format(score))\n",
    "    return oof_pred, models\n",
    "\n",
    "# Catboost\n",
    "def fit_cb(X, y, cv, params: dict=None, verbose: int=50):\n",
    "    metric_func = mean_squared_error\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    models = []\n",
    "    oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "\n",
    "    for i, (idx_train, idx_valid) in enumerate(cv): \n",
    "        x_train, y_train = X[idx_train], y[idx_train]\n",
    "        x_valid, y_valid = X[idx_valid], y[idx_valid]\n",
    "        \n",
    "        train_pool = Pool(x_train, label = y_train)\n",
    "        valid_pool = Pool(x_valid, label = y_valid)\n",
    "        \n",
    "        model_cb = CatBoostRegressor(**params)\n",
    "\n",
    "        with timer(prefix='fit fold={} '.format(i + 1)):\n",
    "            model_cb.fit(train_pool,\n",
    "              # valid_data\n",
    "              eval_set = valid_pool,\n",
    "              use_best_model = True,\n",
    "              silent = True,\n",
    "              plot = False)\n",
    "            \n",
    "        print(model_cb.get_best_score())\n",
    "        \n",
    "        pred_i = model_cb.predict(x_valid)\n",
    "\n",
    "        oof_pred[idx_valid] = pred_i\n",
    "        models.append(model_cb)\n",
    "\n",
    "        print(f'Fold {i} RMSLE: {metric_func(y_valid, pred_i)**.5 :.4f}')\n",
    "\n",
    "    score = metric_func(y, oof_pred)**.5 \n",
    "    print('FINISHED | Whole RMSLE: {:.4f}'.format(score))\n",
    "    return oof_pred, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "recovered-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupKFold:\n",
    "    \"\"\"\n",
    "    GroupKFold with random shuffle with a sklearn-like structure\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=42):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, group=None):\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X=None, y=None, group=None):\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n",
    "        unique_ids = group.unique()\n",
    "        for tr_group_idx, va_group_idx in kf.split(unique_ids):\n",
    "            # split group\n",
    "            tr_group, va_group = unique_ids[tr_group_idx], unique_ids[va_group_idx]\n",
    "            train_idx = np.where(group.isin(tr_group))[0]\n",
    "            val_idx = np.where(group.isin(va_group))[0]\n",
    "            yield train_idx, val_idx\n",
    "\n",
    "\n",
    "# PlaceID をキーにした Group K fold\n",
    "def make_gkf(X, y, n_splits=5, random_state=2020):\n",
    "    gkf = GroupKFold(n_splits=n_splits, random_state=random_state)\n",
    "    return list(gkf.split(X, y, train_df[\"PlaceID\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "compound-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cv =make_gkf(train_x, train_ys)\n",
    "group_cv2 =make_gkf(train_x, train_ys, 5, 71)\n",
    "group_cv3 =make_gkf(train_x, train_ys, 5, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "juvenile-decision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.774816\n",
      "[100]\tvalid_0's rmse: 0.642579\n",
      "[150]\tvalid_0's rmse: 0.586299\n",
      "[200]\tvalid_0's rmse: 0.561054\n",
      "[250]\tvalid_0's rmse: 0.548692\n",
      "[300]\tvalid_0's rmse: 0.542027\n",
      "[350]\tvalid_0's rmse: 0.538238\n",
      "[400]\tvalid_0's rmse: 0.535607\n",
      "[450]\tvalid_0's rmse: 0.534053\n",
      "[500]\tvalid_0's rmse: 0.533171\n",
      "[550]\tvalid_0's rmse: 0.532535\n",
      "[600]\tvalid_0's rmse: 0.532228\n",
      "[650]\tvalid_0's rmse: 0.531672\n",
      "[700]\tvalid_0's rmse: 0.531506\n",
      "[750]\tvalid_0's rmse: 0.531592\n",
      "Early stopping, best iteration is:\n",
      "[717]\tvalid_0's rmse: 0.531379\n",
      "fit fold=1 19.921[s]\n",
      "Fold 0 RMSLE: 0.5314\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.902036\n",
      "[100]\tvalid_0's rmse: 0.722727\n",
      "[150]\tvalid_0's rmse: 0.628569\n",
      "[200]\tvalid_0's rmse: 0.580601\n",
      "[250]\tvalid_0's rmse: 0.554605\n",
      "[300]\tvalid_0's rmse: 0.540055\n",
      "[350]\tvalid_0's rmse: 0.532119\n",
      "[400]\tvalid_0's rmse: 0.527588\n",
      "[450]\tvalid_0's rmse: 0.524288\n",
      "[500]\tvalid_0's rmse: 0.52112\n",
      "[550]\tvalid_0's rmse: 0.519415\n",
      "[600]\tvalid_0's rmse: 0.517717\n",
      "[650]\tvalid_0's rmse: 0.51666\n",
      "[700]\tvalid_0's rmse: 0.515961\n",
      "[750]\tvalid_0's rmse: 0.515414\n",
      "[800]\tvalid_0's rmse: 0.514826\n",
      "[850]\tvalid_0's rmse: 0.514529\n",
      "[900]\tvalid_0's rmse: 0.514193\n",
      "[950]\tvalid_0's rmse: 0.514159\n",
      "[1000]\tvalid_0's rmse: 0.513588\n",
      "[1050]\tvalid_0's rmse: 0.513436\n",
      "[1100]\tvalid_0's rmse: 0.513271\n",
      "[1150]\tvalid_0's rmse: 0.513092\n",
      "[1200]\tvalid_0's rmse: 0.513105\n",
      "Early stopping, best iteration is:\n",
      "[1168]\tvalid_0's rmse: 0.513015\n",
      "fit fold=2 28.212[s]\n",
      "Fold 1 RMSLE: 0.5130\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.816426\n",
      "[100]\tvalid_0's rmse: 0.68343\n",
      "[150]\tvalid_0's rmse: 0.621773\n",
      "[200]\tvalid_0's rmse: 0.591644\n",
      "[250]\tvalid_0's rmse: 0.575794\n",
      "[300]\tvalid_0's rmse: 0.56768\n",
      "[350]\tvalid_0's rmse: 0.564008\n",
      "[400]\tvalid_0's rmse: 0.561325\n",
      "[450]\tvalid_0's rmse: 0.560138\n",
      "[500]\tvalid_0's rmse: 0.558968\n",
      "[550]\tvalid_0's rmse: 0.559008\n",
      "Early stopping, best iteration is:\n",
      "[525]\tvalid_0's rmse: 0.55863\n",
      "fit fold=3 15.356[s]\n",
      "Fold 2 RMSLE: 0.5586\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.833025\n",
      "[100]\tvalid_0's rmse: 0.699573\n",
      "[150]\tvalid_0's rmse: 0.630654\n",
      "[200]\tvalid_0's rmse: 0.596088\n",
      "[250]\tvalid_0's rmse: 0.576677\n",
      "[300]\tvalid_0's rmse: 0.564609\n",
      "[350]\tvalid_0's rmse: 0.557074\n",
      "[400]\tvalid_0's rmse: 0.552223\n",
      "[450]\tvalid_0's rmse: 0.547716\n",
      "[500]\tvalid_0's rmse: 0.54462\n",
      "[550]\tvalid_0's rmse: 0.542499\n",
      "[600]\tvalid_0's rmse: 0.540896\n",
      "[650]\tvalid_0's rmse: 0.539314\n",
      "[700]\tvalid_0's rmse: 0.538148\n",
      "[750]\tvalid_0's rmse: 0.537607\n",
      "[800]\tvalid_0's rmse: 0.537294\n",
      "[850]\tvalid_0's rmse: 0.537098\n",
      "[900]\tvalid_0's rmse: 0.536692\n",
      "[950]\tvalid_0's rmse: 0.536762\n",
      "Early stopping, best iteration is:\n",
      "[900]\tvalid_0's rmse: 0.536692\n",
      "fit fold=4 22.621[s]\n",
      "Fold 3 RMSLE: 0.5367\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.849785\n",
      "[100]\tvalid_0's rmse: 0.704075\n",
      "[150]\tvalid_0's rmse: 0.637681\n",
      "[200]\tvalid_0's rmse: 0.60891\n",
      "[250]\tvalid_0's rmse: 0.597045\n",
      "[300]\tvalid_0's rmse: 0.590014\n",
      "[350]\tvalid_0's rmse: 0.585833\n",
      "[400]\tvalid_0's rmse: 0.583949\n",
      "[450]\tvalid_0's rmse: 0.58205\n",
      "[500]\tvalid_0's rmse: 0.581406\n",
      "[550]\tvalid_0's rmse: 0.580381\n",
      "[600]\tvalid_0's rmse: 0.579458\n",
      "[650]\tvalid_0's rmse: 0.578764\n",
      "[700]\tvalid_0's rmse: 0.578355\n",
      "[750]\tvalid_0's rmse: 0.577908\n",
      "[800]\tvalid_0's rmse: 0.577632\n",
      "[850]\tvalid_0's rmse: 0.577458\n",
      "[900]\tvalid_0's rmse: 0.577208\n",
      "Early stopping, best iteration is:\n",
      "[886]\tvalid_0's rmse: 0.57703\n",
      "fit fold=5 22.281[s]\n",
      "Fold 4 RMSLE: 0.5770\n",
      "FINISHED | Whole RMSLE: 0.5439\n"
     ]
    }
   ],
   "source": [
    "lgm_params = {  \n",
    "    \"n_estimators\": 20000,\n",
    "    \"objective\": 'rmse',\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_leaves\": 36,\n",
    "    \"random_state\": 2021,\n",
    "    \"n_jobs\": -1,\n",
    "    \"importance_type\": \"gain\",\n",
    "    'colsample_bytree': .5,\n",
    "    \"reg_lambda\": 5,\n",
    "    \"max_depth\":7,\n",
    "    }\n",
    "\n",
    "oof, models = fit_lgbm(train_x.values, train_ys,group_cv , params=lgm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "engaging-ultimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.886802\n",
      "[100]\tvalid_0's rmse: 0.726178\n",
      "[150]\tvalid_0's rmse: 0.646051\n",
      "[200]\tvalid_0's rmse: 0.605451\n",
      "[250]\tvalid_0's rmse: 0.585236\n",
      "[300]\tvalid_0's rmse: 0.574223\n",
      "[350]\tvalid_0's rmse: 0.567214\n",
      "[400]\tvalid_0's rmse: 0.563434\n",
      "[450]\tvalid_0's rmse: 0.560632\n",
      "[500]\tvalid_0's rmse: 0.558563\n",
      "[550]\tvalid_0's rmse: 0.557083\n",
      "[600]\tvalid_0's rmse: 0.555698\n",
      "[650]\tvalid_0's rmse: 0.554825\n",
      "[700]\tvalid_0's rmse: 0.554159\n",
      "[750]\tvalid_0's rmse: 0.55385\n",
      "[800]\tvalid_0's rmse: 0.553719\n",
      "[850]\tvalid_0's rmse: 0.553502\n",
      "[900]\tvalid_0's rmse: 0.553286\n",
      "Early stopping, best iteration is:\n",
      "[872]\tvalid_0's rmse: 0.553163\n",
      "fit fold=1 23.297[s]\n",
      "Fold 0 RMSLE: 0.5532\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.917375\n",
      "[100]\tvalid_0's rmse: 0.759833\n",
      "[150]\tvalid_0's rmse: 0.684258\n",
      "[200]\tvalid_0's rmse: 0.648135\n",
      "[250]\tvalid_0's rmse: 0.62935\n",
      "[300]\tvalid_0's rmse: 0.619899\n",
      "[350]\tvalid_0's rmse: 0.61395\n",
      "[400]\tvalid_0's rmse: 0.610824\n",
      "[450]\tvalid_0's rmse: 0.607594\n",
      "[500]\tvalid_0's rmse: 0.605062\n",
      "[550]\tvalid_0's rmse: 0.603846\n",
      "[600]\tvalid_0's rmse: 0.603178\n",
      "[650]\tvalid_0's rmse: 0.602624\n",
      "[700]\tvalid_0's rmse: 0.602263\n",
      "[750]\tvalid_0's rmse: 0.602216\n",
      "[800]\tvalid_0's rmse: 0.601889\n",
      "[850]\tvalid_0's rmse: 0.601753\n",
      "[900]\tvalid_0's rmse: 0.601862\n",
      "Early stopping, best iteration is:\n",
      "[867]\tvalid_0's rmse: 0.60166\n",
      "fit fold=2 21.663[s]\n",
      "Fold 1 RMSLE: 0.6017\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.818036\n",
      "[100]\tvalid_0's rmse: 0.688795\n",
      "[150]\tvalid_0's rmse: 0.627359\n",
      "[200]\tvalid_0's rmse: 0.598269\n",
      "[250]\tvalid_0's rmse: 0.582929\n",
      "[300]\tvalid_0's rmse: 0.573986\n",
      "[350]\tvalid_0's rmse: 0.569676\n",
      "[400]\tvalid_0's rmse: 0.566401\n",
      "[450]\tvalid_0's rmse: 0.564927\n",
      "[500]\tvalid_0's rmse: 0.563973\n",
      "[550]\tvalid_0's rmse: 0.563556\n",
      "[600]\tvalid_0's rmse: 0.56349\n",
      "Early stopping, best iteration is:\n",
      "[584]\tvalid_0's rmse: 0.5634\n",
      "fit fold=3 16.880[s]\n",
      "Fold 2 RMSLE: 0.5634\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.791597\n",
      "[100]\tvalid_0's rmse: 0.636998\n",
      "[150]\tvalid_0's rmse: 0.563437\n",
      "[200]\tvalid_0's rmse: 0.529232\n",
      "[250]\tvalid_0's rmse: 0.51223\n",
      "[300]\tvalid_0's rmse: 0.504482\n",
      "[350]\tvalid_0's rmse: 0.500414\n",
      "[400]\tvalid_0's rmse: 0.497851\n",
      "[450]\tvalid_0's rmse: 0.496925\n",
      "[500]\tvalid_0's rmse: 0.496311\n",
      "[550]\tvalid_0's rmse: 0.496354\n",
      "[600]\tvalid_0's rmse: 0.496064\n",
      "[650]\tvalid_0's rmse: 0.495884\n",
      "Early stopping, best iteration is:\n",
      "[643]\tvalid_0's rmse: 0.495777\n",
      "fit fold=4 17.026[s]\n",
      "Fold 3 RMSLE: 0.4958\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.742386\n",
      "[100]\tvalid_0's rmse: 0.616973\n",
      "[150]\tvalid_0's rmse: 0.559969\n",
      "[200]\tvalid_0's rmse: 0.535017\n",
      "[250]\tvalid_0's rmse: 0.523105\n",
      "[300]\tvalid_0's rmse: 0.518908\n",
      "[350]\tvalid_0's rmse: 0.515811\n",
      "[400]\tvalid_0's rmse: 0.513359\n",
      "[450]\tvalid_0's rmse: 0.511936\n",
      "[500]\tvalid_0's rmse: 0.511473\n",
      "[550]\tvalid_0's rmse: 0.510535\n",
      "[600]\tvalid_0's rmse: 0.509903\n",
      "[650]\tvalid_0's rmse: 0.509203\n",
      "[700]\tvalid_0's rmse: 0.508709\n",
      "[750]\tvalid_0's rmse: 0.508421\n",
      "[800]\tvalid_0's rmse: 0.508742\n",
      "Early stopping, best iteration is:\n",
      "[756]\tvalid_0's rmse: 0.508389\n",
      "fit fold=5 19.554[s]\n",
      "Fold 4 RMSLE: 0.5084\n",
      "FINISHED | Whole RMSLE: 0.5459\n"
     ]
    }
   ],
   "source": [
    "oof_lgb2, models_lgb2 = fit_lgbm(train_x.values, train_ys, group_cv2 , params=lgm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "parallel-official",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.906771\n",
      "[100]\tvalid_0's rmse: 0.756035\n",
      "[150]\tvalid_0's rmse: 0.679403\n",
      "[200]\tvalid_0's rmse: 0.641067\n",
      "[250]\tvalid_0's rmse: 0.619688\n",
      "[300]\tvalid_0's rmse: 0.606533\n",
      "[350]\tvalid_0's rmse: 0.59839\n",
      "[400]\tvalid_0's rmse: 0.593554\n",
      "[450]\tvalid_0's rmse: 0.590801\n",
      "[500]\tvalid_0's rmse: 0.588081\n",
      "[550]\tvalid_0's rmse: 0.586414\n",
      "[600]\tvalid_0's rmse: 0.585412\n",
      "[650]\tvalid_0's rmse: 0.584017\n",
      "[700]\tvalid_0's rmse: 0.58332\n",
      "[750]\tvalid_0's rmse: 0.58284\n",
      "[800]\tvalid_0's rmse: 0.581913\n",
      "[850]\tvalid_0's rmse: 0.581177\n",
      "[900]\tvalid_0's rmse: 0.580432\n",
      "[950]\tvalid_0's rmse: 0.580203\n",
      "[1000]\tvalid_0's rmse: 0.579953\n",
      "[1050]\tvalid_0's rmse: 0.579636\n",
      "[1100]\tvalid_0's rmse: 0.579278\n",
      "[1150]\tvalid_0's rmse: 0.57908\n",
      "[1200]\tvalid_0's rmse: 0.578864\n",
      "[1250]\tvalid_0's rmse: 0.578724\n",
      "[1300]\tvalid_0's rmse: 0.578753\n",
      "Early stopping, best iteration is:\n",
      "[1252]\tvalid_0's rmse: 0.578722\n",
      "fit fold=1 29.358[s]\n",
      "Fold 0 RMSLE: 0.5787\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.72099\n",
      "[100]\tvalid_0's rmse: 0.595358\n",
      "[150]\tvalid_0's rmse: 0.537154\n",
      "[200]\tvalid_0's rmse: 0.51056\n",
      "[250]\tvalid_0's rmse: 0.497627\n",
      "[300]\tvalid_0's rmse: 0.491272\n",
      "[350]\tvalid_0's rmse: 0.487104\n",
      "[400]\tvalid_0's rmse: 0.483736\n",
      "[450]\tvalid_0's rmse: 0.482108\n",
      "[500]\tvalid_0's rmse: 0.480533\n",
      "[550]\tvalid_0's rmse: 0.4795\n",
      "[600]\tvalid_0's rmse: 0.478791\n",
      "[650]\tvalid_0's rmse: 0.477658\n",
      "[700]\tvalid_0's rmse: 0.477032\n",
      "[750]\tvalid_0's rmse: 0.476648\n",
      "[800]\tvalid_0's rmse: 0.476346\n",
      "[850]\tvalid_0's rmse: 0.476318\n",
      "Early stopping, best iteration is:\n",
      "[811]\tvalid_0's rmse: 0.476284\n",
      "fit fold=2 20.896[s]\n",
      "Fold 1 RMSLE: 0.4763\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.831756\n",
      "[100]\tvalid_0's rmse: 0.702902\n",
      "[150]\tvalid_0's rmse: 0.643707\n",
      "[200]\tvalid_0's rmse: 0.616837\n",
      "[250]\tvalid_0's rmse: 0.603882\n",
      "[300]\tvalid_0's rmse: 0.596312\n",
      "[350]\tvalid_0's rmse: 0.592389\n",
      "[400]\tvalid_0's rmse: 0.590614\n",
      "[450]\tvalid_0's rmse: 0.589601\n",
      "[500]\tvalid_0's rmse: 0.588515\n",
      "[550]\tvalid_0's rmse: 0.587227\n",
      "[600]\tvalid_0's rmse: 0.586112\n",
      "[650]\tvalid_0's rmse: 0.585573\n",
      "[700]\tvalid_0's rmse: 0.585272\n",
      "[750]\tvalid_0's rmse: 0.58471\n",
      "[800]\tvalid_0's rmse: 0.584361\n",
      "[850]\tvalid_0's rmse: 0.584357\n",
      "Early stopping, best iteration is:\n",
      "[843]\tvalid_0's rmse: 0.584219\n",
      "fit fold=3 21.565[s]\n",
      "Fold 2 RMSLE: 0.5842\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.860805\n",
      "[100]\tvalid_0's rmse: 0.694098\n",
      "[150]\tvalid_0's rmse: 0.605052\n",
      "[200]\tvalid_0's rmse: 0.560635\n",
      "[250]\tvalid_0's rmse: 0.537544\n",
      "[300]\tvalid_0's rmse: 0.524661\n",
      "[350]\tvalid_0's rmse: 0.516143\n",
      "[400]\tvalid_0's rmse: 0.510557\n",
      "[450]\tvalid_0's rmse: 0.506583\n",
      "[500]\tvalid_0's rmse: 0.504645\n",
      "[550]\tvalid_0's rmse: 0.502491\n",
      "[600]\tvalid_0's rmse: 0.500688\n",
      "[650]\tvalid_0's rmse: 0.499155\n",
      "[700]\tvalid_0's rmse: 0.498132\n",
      "[750]\tvalid_0's rmse: 0.497383\n",
      "[800]\tvalid_0's rmse: 0.496911\n",
      "[850]\tvalid_0's rmse: 0.496178\n",
      "[900]\tvalid_0's rmse: 0.495593\n",
      "[950]\tvalid_0's rmse: 0.495175\n",
      "[1000]\tvalid_0's rmse: 0.494778\n",
      "[1050]\tvalid_0's rmse: 0.494577\n",
      "[1100]\tvalid_0's rmse: 0.494431\n",
      "[1150]\tvalid_0's rmse: 0.494445\n",
      "Early stopping, best iteration is:\n",
      "[1106]\tvalid_0's rmse: 0.494385\n",
      "fit fold=4 26.696[s]\n",
      "Fold 3 RMSLE: 0.4944\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.83198\n",
      "[100]\tvalid_0's rmse: 0.66938\n",
      "[150]\tvalid_0's rmse: 0.590803\n",
      "[200]\tvalid_0's rmse: 0.5569\n",
      "[250]\tvalid_0's rmse: 0.540763\n",
      "[300]\tvalid_0's rmse: 0.533785\n",
      "[350]\tvalid_0's rmse: 0.529564\n",
      "[400]\tvalid_0's rmse: 0.526638\n",
      "[450]\tvalid_0's rmse: 0.524495\n",
      "[500]\tvalid_0's rmse: 0.522957\n",
      "[550]\tvalid_0's rmse: 0.521358\n",
      "[600]\tvalid_0's rmse: 0.520243\n",
      "[650]\tvalid_0's rmse: 0.519774\n",
      "[700]\tvalid_0's rmse: 0.519115\n",
      "[750]\tvalid_0's rmse: 0.518627\n",
      "[800]\tvalid_0's rmse: 0.518203\n",
      "[850]\tvalid_0's rmse: 0.517622\n",
      "[900]\tvalid_0's rmse: 0.51735\n",
      "[950]\tvalid_0's rmse: 0.517426\n",
      "Early stopping, best iteration is:\n",
      "[915]\tvalid_0's rmse: 0.517271\n",
      "fit fold=5 24.195[s]\n",
      "Fold 4 RMSLE: 0.5173\n",
      "FINISHED | Whole RMSLE: 0.5319\n"
     ]
    }
   ],
   "source": [
    "oof_lgb3, models_lgb3 = fit_lgbm(train_x.values, train_ys, group_cv3 , params=lgm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "divine-opinion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit fold=1 275.551[s]\n",
      "{'learn': {'RMSE': 0.038081387717376645}, 'validation': {'RMSE': 0.5292038555969685}}\n",
      "Fold 0 RMSLE: 0.5292\n",
      "fit fold=2 277.454[s]\n",
      "{'learn': {'RMSE': 0.039919377108343576}, 'validation': {'RMSE': 0.52063268870213}}\n",
      "Fold 1 RMSLE: 0.5206\n",
      "fit fold=3 277.837[s]\n",
      "{'learn': {'RMSE': 0.038318644618509}, 'validation': {'RMSE': 0.5552293181402578}}\n",
      "Fold 2 RMSLE: 0.5552\n",
      "fit fold=4 278.456[s]\n",
      "{'learn': {'RMSE': 0.038390233328619454}, 'validation': {'RMSE': 0.538640816713981}}\n",
      "Fold 3 RMSLE: 0.5386\n",
      "fit fold=5 277.980[s]\n",
      "{'learn': {'RMSE': 0.03804498663929287}, 'validation': {'RMSE': 0.5691340492006972}}\n",
      "Fold 4 RMSLE: 0.5691\n",
      "FINISHED | Whole RMSLE: 0.5429\n"
     ]
    }
   ],
   "source": [
    "cb_params = {\n",
    "    'loss_function': 'RMSE',\n",
    "    'num_boost_round': 10000,\n",
    "    'depth':7,\n",
    "    'learning_rate':0.01,\n",
    "    \"random_state\": 2021,\n",
    "    }\n",
    "\n",
    "oof_cb, models_cb = fit_cb(train_x.values, train_ys,group_cv , params=cb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-amino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit fold=1 278.235[s]\n",
      "{'learn': {'RMSE': 0.03936371031462618}, 'validation': {'RMSE': 0.5494590505841918}}\n",
      "Fold 0 RMSLE: 0.5495\n",
      "fit fold=2 276.255[s]\n",
      "{'learn': {'RMSE': 0.03828292978694559}, 'validation': {'RMSE': 0.599951119057503}}\n",
      "Fold 1 RMSLE: 0.6000\n",
      "fit fold=3 277.021[s]\n",
      "{'learn': {'RMSE': 0.038658509903143776}, 'validation': {'RMSE': 0.5637353311454404}}\n",
      "Fold 2 RMSLE: 0.5637\n"
     ]
    }
   ],
   "source": [
    "oof_cb2, models_cb2 = fit_cb(train_x.values, train_ys,group_cv2 , params=cb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_cb3, models_cb3 = fit_cb(train_x.values, train_ys,group_cv3 , params=cb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'num_boost_round': 10000,\n",
    "    'max_depth':7,\n",
    "    'eta':0.03,\n",
    "    \"random_state\": 2021,\n",
    "    \"verbosity\":1\n",
    "    }\n",
    "\n",
    "oof_xgb, models_xgb = fit_xgb(train_x.values, train_ys,group_cv , params=xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_xgb2, models_xgb2 = fit_xgb(train_x.values, train_ys, group_cv2 , params=xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_xgb3, models_xgb3 = fit_xgb(train_x.values, train_ys, group_cv3 , params=xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_importance(models, feat_train_df):\n",
    "    \"\"\"lightGBM の model 配列の feature importance を plot する\n",
    "    CVごとのブレを boxen plot として表現します.\n",
    "\n",
    "    args:\n",
    "        models:\n",
    "            List of lightGBM models\n",
    "        feat_train_df:\n",
    "            学習時に使った DataFrame\n",
    "    \"\"\"\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    for i, model in enumerate(models):\n",
    "        _df = pd.DataFrame()\n",
    "        _df['feature_importance'] = model.feature_importances_\n",
    "        _df['column'] = feat_train_df.columns\n",
    "        _df['fold'] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, _df], axis=0, ignore_index=True)\n",
    "\n",
    "    order = feature_importance_df.groupby('column')\\\n",
    "        .sum()[['feature_importance']]\\\n",
    "        .sort_values('feature_importance', ascending=False).index[:50]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(max(6, len(order) * .4), 7))\n",
    "    sns.boxenplot(data=feature_importance_df, x='column', y='feature_importance', order=order, ax=ax, palette='viridis')\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    ax.grid()\n",
    "    fig.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = visualize_importance(models, train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-white",
   "metadata": {},
   "source": [
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict lgb\n",
    "pred_lgb1 = np.array([model.predict(test_x.values) for model in models])\n",
    "pred_lgb1 = np.mean(pred_lgb1, axis=0)\n",
    "pred_lgb1 = np.where(pred_lgb1 < 0, 0, pred_lgb1)\n",
    "pred_lgb1 = np.expm1(pred_lgb1)\n",
    "\n",
    "pred_lgb2 = np.array([model.predict(test_x.values) for model in models_lgb2])\n",
    "pred_lgb2 = np.mean(pred_lgb2, axis=0)\n",
    "pred_lgb2 = np.where(pred_lgb2 < 0, 0, pred_lgb2)\n",
    "pred_lgb2 = np.expm1(pred_lgb2)\n",
    "\n",
    "pred_lgb3 = np.array([model.predict(test_x.values) for model in models_lgb3])\n",
    "pred_lgb3 = np.mean(pred_lgb3, axis=0)\n",
    "pred_lgb3 = np.where(pred_lgb3 < 0, 0, pred_lgb3)\n",
    "pred_lgb3 = np.expm1(pred_lgb3)\n",
    "\n",
    "# seed average lgb\n",
    "pred_lgb = (pred_lgb1+pred_lgb2+pred_lgb3)/3\n",
    "\n",
    "# predict catboost\n",
    "pred_cb1 = np.array([model.predict(test_x.values) for model in models_cb])\n",
    "pred_cb1 = np.mean(pred_cb1, axis=0)\n",
    "pred_cb1 = np.where(pred_cb1 < 0, 0, pred_cb1)\n",
    "pred_cb1 = np.expm1(pred_cb1)\n",
    "\n",
    "pred_cb2 = np.array([model.predict(test_x.values) for model in models_cb2])\n",
    "pred_cb2 = np.mean(pred_cb2, axis=0)\n",
    "pred_cb2 = np.where(pred_cb2 < 0, 0, pred_cb2)\n",
    "pred_cb2 = np.expm1(pred_cb2)\n",
    "\n",
    "pred_cb3 = np.array([model.predict(test_x.values) for model in models_cb3])\n",
    "pred_cb3 = np.mean(pred_cb3, axis=0)\n",
    "pred_cb3 = np.where(pred_cb3 < 0, 0, pred_cb3)\n",
    "pred_cb3 = np.expm1(pred_cb3)\n",
    "\n",
    "# seed average catboost\n",
    "pred_cb = (pred_cb1+pred_cb2+pred_cb3)/3\n",
    "\n",
    "# predict xgb\n",
    "pred_xgb1 = np.array([model.predict(test_x.values) for model in models_xgb])\n",
    "pred_xgb1 = np.mean(pred_xgb1, axis=0)\n",
    "pred_xgb1 = np.where(pred_xgb1 < 0, 0, pred_xgb1)\n",
    "pred_xgb1 = np.expm1(pred_xgb1)\n",
    "\n",
    "pred_xgb2 = np.array([model.predict(test_x.values) for model in models_xgb2])\n",
    "pred_xgb2 = np.mean(pred_xgb2, axis=0)\n",
    "pred_xgb2 = np.where(pred_xgb2 < 0, 0, pred_xgb2)\n",
    "pred_xgb2 = np.expm1(pred_xgb2)\n",
    "\n",
    "pred_xgb3 = np.array([model.predict(test_x.values) for model in models_xgb3])\n",
    "pred_xgb3 = np.mean(pred_xgb3, axis=0)\n",
    "pred_xgb3 = np.where(pred_xgb3 < 0, 0, pred_xgb3)\n",
    "pred_xgb3 = np.expm1(pred_xgb3)\n",
    "\n",
    "# seed average catboost\n",
    "pred_xgb = (pred_xgb1+pred_xgb2+pred_xgb3)/3\n",
    "\n",
    "\n",
    "pred_em = (pred_lgb+pred_cb+pred_xgb)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "date=\"20210402_2\"\n",
    "submission[\"LandPrice\"] = pred_lgb\n",
    "submission.to_csv(output_dir + \"date\" + date +'_simple_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.histplot(np.log1p(pred_lgb), label='Test Predict', ax=ax, color='black')\n",
    "sns.histplot(oof, label='Out Of Fold', ax=ax, color='C1')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-failure",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
